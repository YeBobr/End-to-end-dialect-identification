# rnnlm related
layer: 2
unit: 1024
opt: sgd        # or adam
batchsize: 64   # batch size in LM training
epoch: 30     # if the data size is large, we can reduce this
patience: 3
maxlen: 100     # if sentence length > lm_maxlen, lm_batchsize is automatically reduced
